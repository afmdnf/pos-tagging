{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging using Maximum Entropy Markov Models\n",
    "A first-order maximum entropy Markov model will be implemented for the purpose of part-of-speech tagging. In the process, we will implement Viterbi decoding and featurize the text for the MEMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import linear_model\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "\n",
    "# Dictionary to store indices for each feature\n",
    "feature_vocab = {}\n",
    "\n",
    "# Dictionary to store the indices of each POS tag\n",
    "label_vocab = {}\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "        load data from filename and return a list of lists\n",
    "        all_toks = [toks1, toks2, ...] where toks1 is\n",
    "                a sequence of words (sentence)\n",
    "\n",
    "        all_labs = [labs1, labs2, ...] where labs1 is a sequence of\n",
    "                labels for the corresponding sentence\n",
    "    \"\"\"\n",
    "    file = open(filename)\n",
    "    all_toks = []\n",
    "    all_labs = []\n",
    "    toks = []\n",
    "    labs = []\n",
    "    for line in file:\n",
    "        if \"This data is licensed from\" in line:\n",
    "            continue\n",
    "        cols = line.rstrip().split(\"\\t\")\n",
    "        if len(cols) < 2:\n",
    "            all_toks.append(toks)\n",
    "            all_labs.append(labs)\n",
    "            toks = []\n",
    "            labs = []\n",
    "            continue\n",
    "        toks.append(cols[0])\n",
    "        labs.append(cols[1])\n",
    "\n",
    "    if len(toks) > 0:\n",
    "        all_toks.append(toks)\n",
    "        all_labs.append(labs)\n",
    "\n",
    "    return all_toks, all_labs\n",
    "\n",
    "def decode(Y_pred):\n",
    "    if use_greedy:\n",
    "        return greedy_decode(Y_pred)\n",
    "    else:\n",
    "        return viterbi_decode(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell there are some constants that will be used for the MEMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control variables, use for debugging and testing\n",
    "verbose = True\n",
    "use_greedy = False\n",
    "\n",
    "# Minimum number of observations for a feature to be included in model\n",
    "MINIMUM_FEAT_COUNT = 2 \n",
    "# L2 regularization strength; range is (0,infinity), with stronger regularization -> 0 (in this package)\n",
    "L2_REGULARIZATION_STRENGTH = 0.9 \n",
    "\n",
    "TRAINING_FILE = 'wsj.pos.train'\n",
    "TEST_FILE = 'wsj.pos.test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decoding\n",
    "Following are functions that perform decoding: the trivial greedy approach and a Viterbi decoding implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(Y_pred):\n",
    "    \"\"\" \n",
    "        greedy decoding to get the sequence of label predictions\n",
    "    \"\"\"\n",
    "    cur = label_vocab[\"START\"]\n",
    "    preds = []\n",
    "    for i in range(len(Y_pred)):\n",
    "        pred = np.argmax(Y_pred[i, cur])\n",
    "        preds.append(pred)\n",
    "        cur = pred\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decode(Y_pred):\n",
    "    \"\"\"\n",
    "        :return\n",
    "        list of POS tag indices, defined in label_vocab,\n",
    "        in order of sequence\n",
    "\n",
    "        :param\n",
    "        Y_pred: Tensor of shape N * M * L, where\n",
    "        N is the number of words in the current sequence,\n",
    "        L is the number of POS tag labels\n",
    "        M = L + 1, where M is the number of possible previous labels\n",
    "        which includes L + \"START\"\n",
    "\n",
    "        M_{x,y,z} is the probability of a tag (label_vocab[current_tag] = z)\n",
    "        given its current position x in the sequence and\n",
    "        its previous tag (label_vocab[previous_tag] = y)\n",
    "\n",
    "        Consider the sentence - \"I have a dog\". Here, N = 4.\n",
    "        Assume that there are only 3 possible tags: {PRP, VBD, NN}\n",
    "        M_{0, 3, 2} would give you the probability of \"I\" being a \"NN\" if\n",
    "        it is preceded by \"START\". \"START\" is the last index of all lablels,\n",
    "        and in our example denoted by 3.\n",
    "    \"\"\"\n",
    "    (N, M, L) = Y_pred.shape\n",
    "\n",
    "    # list of POS tag indices to be returned\n",
    "    path = []\n",
    "\n",
    "    \"\"\"\n",
    "        step 1. construct a Viterbi matrix to store the\n",
    "        intermediate joint probabilities\n",
    "        step 2. construct a matrix to store the backpointers\n",
    "        to retrieve the path\n",
    "        step 3. decode and return the path from the Viterbi matrix\n",
    "    \"\"\"\n",
    "    viterbi_matrix = np.zeros(shape=(N, L)) \n",
    "    bps = np.zeros(shape=(N, L), dtype=np.int32)\n",
    "    for word in range(N):\n",
    "        if word == 0: # START\n",
    "            bps[word] = np.array([Y_pred[0].shape[0] - 1] * L)\n",
    "            viterbi_matrix[word] = np.log(Y_pred[word, Y_pred[0].shape[0] - 1])\n",
    "        else:\n",
    "            for tag in range(L):\n",
    "                viterbi = [viterbi_matrix[word-1, prev_tag] + np.log(Y_pred[word, prev_tag, tag]) for prev_tag in range(L)]\n",
    "                bps[word, tag] = np.argmax(viterbi)\n",
    "                viterbi_matrix[word, tag] = np.max(viterbi)\n",
    "    \n",
    "    # start off with node with highest value\n",
    "    node = np.argmax(viterbi_matrix[N-1])\n",
    "    path.append(node)\n",
    "    \n",
    "    # tracing the backpointers to form path\n",
    "    i = N - 1\n",
    "    while i > 0:\n",
    "        path.append(bps[i, node])\n",
    "        node = bps[i, node]\n",
    "        i -= 1\n",
    "\n",
    "    return path[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features\n",
    "The `get_features` function adds features for the maximum entropy Markov model to use. The features are explained in detail in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(index, sequence, tag_index_1):\n",
    "    \"\"\"\n",
    "        :params\n",
    "        index: the index of the current word in the sequence to featurize\n",
    "\n",
    "        sequence: the sequence of words for the entire sentence\n",
    "\n",
    "        tag_index_1: gives you the POS\n",
    "        tag for the previous word.\n",
    "\n",
    "        :return (feature dictionary)\n",
    "        features are in the form of {feature_name: feature_value}\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    features[\"UNIGRAM_%s\" % sequence[index].lower()] = 1\n",
    "    features[\"PREVIOUS_TAG_%s\" % tag_index_1] = 1\n",
    "\n",
    "    # Feature Class 1: Miscellaneous --------------------------------------------------\n",
    "    \n",
    "    features[\"WORD_{}_PREVTAG_{}\".format(sequence[index].lower(), tag_index_1)] = 1\n",
    "    \n",
    "    if sequence[index].strip()[0].isupper():\n",
    "        if all(c.isupper() for c in sequence[index].strip()):\n",
    "            features[\"ALL_CAPITAL\"] = 1\n",
    "        else:\n",
    "            features[\"CAPITALIZED\"] = 1\n",
    "   \n",
    "    if any(c.isdigit() for c in sequence[index]):\n",
    "        features[\"CONTAINS_NUMBER\"] = 1\n",
    "    \n",
    "    if '-' in sequence[index]:\n",
    "        features['HYPHENATED'] = 1\n",
    "    \n",
    "    # Feature Class 2: Bigrams --------------------------------------------------\n",
    "\n",
    "    if index != 0: # past word bigram\n",
    "        features[\"BIGRAM_{0}_{1}\".format(sequence[index].lower(), sequence[index-1].lower())] = 1\n",
    "    else:\n",
    "        features[\"FIRST_WORD_IN_SEQUENCE\"] = 1\n",
    "\n",
    "    if index != len(sequence) - 1: # future word bigram\n",
    "        features[\"BIGRAM_{0}_{1}\".format(sequence[index].lower(), sequence[index+1].lower())] = 1\n",
    "    else:\n",
    "        features[\"LAST_WORD_IN_SEQUENCE\"] = 1\n",
    "        \n",
    "    # Feature Class 3: Trigrams --------------------------------------------------\n",
    "        \n",
    "    if index >= 1 and index < len(sequence) - 1: # trigram of past and future words\n",
    "        w1 = sequence[index].lower()\n",
    "        w2 = sequence[index-1].lower()\n",
    "        w3 = sequence[index+1].lower()\n",
    "        features[\"TRIGRAM_{0}_{1}_{2}\".format(w2, w1, w3)] = 1\n",
    "        \n",
    "    # Feature Class 4: Prefixes --------------------------------------------------\n",
    "    \n",
    "    chk = sequence[index].strip()\n",
    "    \n",
    "    features[\"PREFIX_{0}\".format(sequence[index]).lower()[:3]] = 1\n",
    "    \n",
    "    if chk.startswith(\"anti\"):\n",
    "        features['STARTS_anti'] = 1\n",
    "    \n",
    "    if chk.startswith(\"pre\"):\n",
    "        features['STARTS_pre'] = 1\n",
    "    \n",
    "    if chk.startswith(\"dis\"):\n",
    "        features['STARTS_dis'] = 1\n",
    "    \n",
    "    if chk.startswith(\"mis\"):\n",
    "        features['STARTS_mis'] = 1\n",
    "    \n",
    "    if chk.startswith(\"inter\"):\n",
    "        features['STARTS_inter'] = 1\n",
    "    \n",
    "    if chk.startswith(\"un\"):\n",
    "        features['STARTS_un'] = 1\n",
    "        \n",
    "    if chk.startswith(\"non\"):\n",
    "        features['STARTS_non'] = 1\n",
    "    \n",
    "    if chk.startswith(\"over\") or chk.startswith(\"under\"):\n",
    "        features['STARTS_over_under'] = 1\n",
    "    \n",
    "    if chk.startswith(\"in\") or chk.startswith(\"im\"):\n",
    "        features['STARTS_in_im'] = 1\n",
    "    \n",
    "    if chk.startswith(\"en\") or chk.startswith(\"em\"):\n",
    "        features['STARTS_en_em'] = 1\n",
    "   \n",
    "    # Feature Class 5: Suffixes --------------------------------------------------\n",
    "    \n",
    "    features[\"SUFFIX_{0}\".format(sequence[index]).lower()[-2:]] = 1\n",
    "    \n",
    "    if chk.endswith(\"ly\"):\n",
    "        features['ENDS_ly'] = 1\n",
    "    \n",
    "    if chk.endswith(\"ing\"):\n",
    "        features['ENDS_ing'] = 1\n",
    "    \n",
    "    if chk.endswith(\"ness\"):\n",
    "        features['ENDS_ness'] = 1\n",
    "        \n",
    "    if chk.endswith(\"ity\"):\n",
    "        features['ENDS_ity'] = 1\n",
    "    \n",
    "    if chk.endswith(\"ed\"):\n",
    "        features['ENDS_ed'] = 1\n",
    "    \n",
    "    if chk.endswith(\"able\"):\n",
    "        features['ENDS_able'] = 1\n",
    "    \n",
    "    if chk.endswith(\"s\"):\n",
    "        features['ENDS_s'] = 1\n",
    "        \n",
    "    if chk.endswith(\"ion\"):\n",
    "        features['ENDS_ion'] = 1\n",
    "        \n",
    "    if chk.endswith(\"al\"):\n",
    "        features['ENDS_al'] = 1\n",
    "    \n",
    "    if chk.endswith(\"ive\"):\n",
    "        features['ENDS_ive'] = 1\n",
    "    \n",
    "    if chk.endswith(\"er\"):\n",
    "        features['ENDS_er'] = 1\n",
    "    \n",
    "    if chk.endswith(\"est\"):\n",
    "        features['ENDS_est'] = 1\n",
    "    \n",
    "    if chk.endswith(\"day\"):\n",
    "        features['ENDS_day'] = 1\n",
    "        \n",
    "    # Feature Class 6: Specials --------------------------------------------------\n",
    "    \n",
    "    if sequence[index].strip()[0].isupper():\n",
    "        copy_back = index - 1\n",
    "        copy_front = index + 1\n",
    "        while copy_back >= 1 and (index-copy_back) <= 3:\n",
    "            chk = sequence[copy_back].strip().lower()\n",
    "            if \"co.\" in chk or \"inc.\" in chk or \"corp\" in chk or \"company\" in chk or \"llc\" in chk or \"ltd\" in chk:\n",
    "                features['COMPANY'] = 1\n",
    "                break\n",
    "            copy_back -= 1\n",
    "        while copy_front < len(sequence) and (copy_front-index) <= 3:\n",
    "            chk = sequence[copy_front].strip().lower()\n",
    "            if \"co.\" in chk or \"inc.\" in chk or \"corp\" in chk or \"company\" in chk or \"llc\" in chk or \"ltd\" in chk:\n",
    "                features['COMPANY'] = 1\n",
    "                break\n",
    "            copy_front += 1\n",
    "        \n",
    "    if index > 1:\n",
    "        chk = sequence[index - 1].strip().lower()\n",
    "        if chk == \"a\" or chk == \"an\" or chk == \"the\":\n",
    "            features['PREV_DETERMINER'] = 1\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both Viterbi decoding implemented and features added, we can now build and train our MEMM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(filename):\n",
    "    \"\"\"\n",
    "        train a model to generate Y_pred\n",
    "    \"\"\"\n",
    "    all_toks, all_labs = load_data(filename)\n",
    "    vocab = {}\n",
    "\n",
    "    # X_verbose is a list of feature objects for the entire train dataset.\n",
    "    # Each feature object is a dictionary defined by get_features and corresponds to a word.\n",
    "    # Y_verbose is a list of labels for all words in the entire train dataset\n",
    "    X_verbose = []\n",
    "    Y_verbose = []\n",
    "\n",
    "    feature_counts=Counter()\n",
    "\n",
    "    for i in range(len(all_toks)):\n",
    "        toks = all_toks[i]\n",
    "        labs = all_labs[i]\n",
    "        for j in range(len(toks)):\n",
    "            prev_lab = labs[j - 1] if j > 0 else \"START\"\n",
    "            feats = get_features(j, toks, prev_lab)\n",
    "            X_verbose.append(feats)\n",
    "            Y_verbose.append(labs[j])\n",
    "\n",
    "            for feat in feats:\n",
    "                feature_counts[feat]+=1\n",
    "\n",
    "    # construct label_vocab (dictionary) and feature_vocab (dictionary)\n",
    "    # label_vocab[pos_tag_label] = index_for_the_pos_tag\n",
    "    # feature_vocab[feature_name] = index_for_the_feature\n",
    "    feature_id = 1\n",
    "    label_id = 0\n",
    "\n",
    "    # create unique integer ids for each label and each feature above the minimum count threshold\n",
    "    for i in range(len(X_verbose)):\n",
    "        feats = X_verbose[i]\n",
    "        true_label = Y_verbose[i]\n",
    "\n",
    "        for feat in feats:\n",
    "            if feature_counts[feat] >= MINIMUM_FEAT_COUNT:\n",
    "                if feat not in feature_vocab:\n",
    "                    feature_vocab[feat] = feature_id\n",
    "                    feature_id += 1\n",
    "        if true_label not in label_vocab:\n",
    "            label_vocab[true_label] = label_id\n",
    "            label_id += 1\n",
    "\n",
    "    # START has last id\n",
    "    label_vocab[\"START\"] = label_id\n",
    "\n",
    "    # create train input and output to train the logistic regression model\n",
    "    # create sparse input matrix\n",
    "\n",
    "    # X is documents x features empty sparse matrix\n",
    "    X = sparse.lil_matrix((len(X_verbose), feature_id))\n",
    "    Y = []\n",
    "\n",
    "    print(\"Number of features: %s\" %len(feature_vocab))\n",
    "    for i in range(len(X_verbose)):\n",
    "        feats = X_verbose[i]\n",
    "        true_label = Y_verbose[i]\n",
    "        for feat in feats:\n",
    "            if feat in feature_vocab:\n",
    "                X[i, feature_vocab[feat]] = feats[feat]\n",
    "        Y.append(label_vocab[true_label])\n",
    "\n",
    "    # fit model\n",
    "    log_reg = linear_model.LogisticRegression(C=L2_REGULARIZATION_STRENGTH, penalty='l2')\n",
    "    log_reg.fit(sparse.coo_matrix(X), Y)\n",
    "\n",
    "    return log_reg\n",
    "\n",
    "\n",
    "def test_dev(filename, log_reg):\n",
    "    \"\"\"\n",
    "        predict labels using the trained model\n",
    "        and evaluate the performance of model\n",
    "    \"\"\"\n",
    "    all_toks, all_labs = load_data(filename)\n",
    "\n",
    "    # possible output labels = all except START\n",
    "    L = len(label_vocab) - 1\n",
    "\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    num_features = len(feature_vocab) + 1\n",
    "\n",
    "    # for each sequence (sentence) in the test dataset\n",
    "    for i in range(len(all_toks)):\n",
    "\n",
    "        toks = all_toks[i]\n",
    "        labs = all_labs[i]\n",
    "\n",
    "        if len(toks) == 0:\n",
    "            continue\n",
    "\n",
    "        N = len(toks)\n",
    "\n",
    "        X_test = []\n",
    "        # N x prev_tag x cur_tag\n",
    "        Y_pred = np.zeros((N, L + 1, L))\n",
    "\n",
    "        # vector of true labels\n",
    "        Y_test = []\n",
    "\n",
    "        # for each token (word) in the sentence\n",
    "        for j in range(len(toks)):\n",
    "\n",
    "            true_label = labs[j]\n",
    "            Y_test.append(true_label)\n",
    "\n",
    "            # for each preceding tag of the word\n",
    "            for possible_previous_tag in label_vocab:\n",
    "                X = sparse.lil_matrix((1, num_features))\n",
    "\n",
    "                feats = get_features(j, toks, possible_previous_tag)\n",
    "                valid_feats = {}\n",
    "                for feat in feats:\n",
    "                    if feat in feature_vocab:\n",
    "                        X[0, feature_vocab[feat]] = feats[feat]\n",
    "\n",
    "                # update Y_pred with the probabilities of all current tags\n",
    "                # given the current word, previous tag and other data/feature\n",
    "                prob = log_reg.predict_proba(X)\n",
    "                Y_pred[j, label_vocab[possible_previous_tag]] = prob\n",
    "\n",
    "        # decode to get the predictions\n",
    "        predictions = decode(Y_pred)\n",
    "\n",
    "        # evaluate the performance of the model by checking predictions\n",
    "        # against true labels\n",
    "        for k in range(len(predictions)):\n",
    "            if Y_test[k] in label_vocab and predictions[k] == label_vocab[Y_test[k]]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "        print(\"Development Accuracy: %.3f (%s/%s).\" % (correct / total, correct, total), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Model\n",
      "Number of features: 291877\n",
      "Test Model\n",
      "Development Accuracy: 0.963 (178867.0/185778.0).\r"
     ]
    }
   ],
   "source": [
    "print(\"Train Model\")\n",
    "log_reg = train(TRAINING_FILE)\n",
    "print(\"Test Model\")\n",
    "test_dev(TEST_FILE, log_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
